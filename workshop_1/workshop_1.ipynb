{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8df0bbd",
   "metadata": {},
   "source": [
    "# Workshop 1\n",
    "In this exercise, we will explore the capabilities of LLMs for natural language processing (NLP) tasks using the Hugging Face (HF) ecosystem. First, we will use the `sentence-transformers` package to extract features from text data by running language models from HF in your own environment. Second, we will use the HF `InferenceClient` API to generate text by running language models hosted on HF servers. \n",
    "\n",
    "By the end of this exercise, you will have learned how to:\n",
    "- Extract features (embeddings) from text data using LLMs via `sentence-transformers`\n",
    "- Generate text using LLMs via the HF API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979416b3c30fb86b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Using Notebook Environments \n",
    "1. To run a cell, press `shift + enter`. The notebook will execute the code in the cell and move to the next cell. If the cell contains a markdown cell (text only), it will render the markdown and move to the next cell.\n",
    "2. Since cells can be executed in any order and variables can be over-written, you may at some point feel that you have lost track of the state of your notebook. If this is the case, you can always restart the notebook by clicking Runtime in the menu bar (if you're using Colab) and selecting `Restart runtime`. This will clear all variables and outputs.\n",
    "3. The final variable in a cell will be printed on the screen. If you want to print preceding variables, use the `print()` function as usual.\n",
    "\n",
    "Notebook environments support code cells and markdown (text) cells. For the purposes of this workshop, markdown cells are used to provide high-level explanations of the code. More specific details are provided in the code cells themselves in the form of comments (lines beginning with `#`)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Environment Setup"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c616368742ccde73"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import sys\n",
    "if 'google.colab' in sys.modules:  # If in Google Colab environment\n",
    "    \n",
    "    # Installing requisite packages\n",
    "    !pip install --upgrade transformers sentence-transformers &> /dev/null"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "290c24c2ed829f80",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We begin by loading the requisite packages. For those coming from R, packages in Python are sometimes given shorter names for use in the code via the `import <name> as <nickname>` syntax (e.g. `import pandas as pd`). The nicknames are usually standardized. We here make use three packages:\n",
    "\n",
    "1. `pandas`: A very popular package for reading and manipulating data in python.\n",
    "2. `sentence_transformers`: A module for extracting features from text data using LLMs.\n",
    "3. `huggingface_hub`: A high-level API to interact with models hosted on the HF Hub. "
   ]
  },
  {
   "cell_type": "code",
   "id": "267fad62d6f82855",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from huggingface_hub import InferenceClient\n",
    "import textwrap"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f6bf097acf12ea11",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Feature Extraction with `sentence_transformers`\n",
    "\n",
    "The following begins by extracting features (embeddings) from the text data---numerical representations of the meaning of text---using the `sentence_transformers` package. To start, it uses three sentences that the code cell places in a list of strings. This list is provided as input to the model. \n",
    "\n",
    "The code makes use of the `all-MiniLM-L6-v2` model, which is a small and efficient embedding model, to extract features from the sentences. The model will encode the sentences into 384-dimensional vector representations. The cell will then print the features as a pandas dataframe for easy viewing. \n",
    "\n",
    "Run the cell below. "
   ]
  },
  {
   "cell_type": "code",
   "id": "98ca015d4695f403",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Define sentences\n",
    "sentences = [\n",
    "    \"I feel great this morning\",\n",
    "    \"I am feeling very good today\",\n",
    "    \"I am feeling terrible\"\n",
    "]\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Extract features\n",
    "features = model.encode(sentences)\n",
    "\n",
    "# Print the features as a pandas dataframe\n",
    "pd.DataFrame(features, index=sentences)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "80664214bd695bf0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**TASK 1**: Have a scroll through the features printed by the cell. Can you see that the features of the first two sentences are more similar to each other (i.e., have similar numerical values) than they are to the third sentence? Why do you think this is the case?\n",
    "\n",
    "**TASK 2**: Try to add another sentence to the `sentences` list defined above by copy-pasting one of the existing sentences but replacing one or two words with a synonym. For instance, you could change \"I feel *great* this morning\" to \"I feel *fantastic* this morning\". Then rerun the cell. What do you notice about the features of this new sentence compared to the original?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc8d5804ede6511",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Text Generation with `huggingface_hub`\n",
    "This section demonstrates how to use the HF API. The main benefit of the API is that it allows us to run the latest, largest open models without having the specialised hardware needed to run them (since the models are run on the cloud). We will use the `meta-llama/Meta-Llama-3-8B-Instruct` to start with (we will show you how to use the larger 70B model in Workshop 3).  \n",
    "\n",
    "The code begins by initializing the `InferenceClient` with an access token, **which you will need to replace with your own [access token](https://huggingface.co/settings/tokens)** (access tokens start with 'hf_...'). \n",
    "\n",
    "It is common for large text-generation models to take a \"system-user\" (or \"system-user-assistant\") prompting format. The format begins with `\"system\"` and then alternates between `\"user\"` and `\"assistant\"` roles to generate a chat-like conversation. In this case, the \"system\" prompt provides the general role that the model should play, and the \"user\" prompt provides the task-specific details. The optional `\"assistant\"` role can be used to add past model responses to the prompt.\n",
    "\n",
    "Run the cell below.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Initialize client\n",
    "api_key = '<your_access_token>' \n",
    "client = InferenceClient(token=api_key)\n",
    "\n",
    "# Create prompts\n",
    "system_role = \"You are a helpful assistant.\"\n",
    "user_role = \"\"\"\n",
    "    Summarize the following text:\n",
    "    \n",
    "    Once upon a time in a land far far away, there was a young prince named John. He was known for his bravery and courage. \n",
    "    One day, he decided to go on an adventure to explore the unknown lands. The prince rode his horse through the dense forests,\n",
    "    crossed the vast deserts, and climbed the highest mountains. After many days of travel, he finally reached the edge of the world.\n",
    "    There, he found a magical portal that led to a parallel universe. The prince stepped through the portal and found himself in a\n",
    "    world filled with strange creatures and mystical beings. He knew that he had found his true calling and decided to stay in this\n",
    "    new world forever.\n",
    "\"\"\"\n",
    "\n",
    "# Feed prompts into model\n",
    "output = client.chat_completion(\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_role},\n",
    "        {\"role\": \"user\", \"content\": user_role}\n",
    "    ],\n",
    "    model=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    max_tokens=50\n",
    ")\n",
    "\n",
    "# Accessing the text in the output object\n",
    "text = output.choices[0].message.content\n",
    "\n",
    "# Printing the output in a more readable format\n",
    "print('\\n'.join(textwrap.wrap(text, 100)))"
   ],
   "id": "9a67e494741df71e",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d4a24c50147f2cf7",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**TASK 1**: Try copy-pasting an abstract of one our your papers into the `user_role` variable and rerun the cell. Does LLama-3 do a good job of summarizing the work?<br>\n",
    "**TASK 2**: Change the `system_role` to `\"You are an incredibly unhelpful assistant.\"` and rerun the cell. Note the power of the system prompt for directing the model's behavior.<br>\n",
    "**TASK 3**: Change the `system_role` back to `\"You are a helpful assistant.\"`. Now try changing the `max_tokens` parameter and see how it affects the length of the summaries.<br>\n",
    "**TASK 4**: Play around, experiment, and have fun with the model! \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
