{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Workshop 3\n",
    "\n",
    "This exercise demonstrates the power and flexibility of text generation. By the end of the exercise, you will have learned to:\n",
    "- Use LLMs for zer-shot text classification\n",
    "- Use the latest, largest open LLMs as synthetic participants in psychological experiments"
   ],
   "id": "b869db79a8e2b860"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Environment Setup ",
   "id": "ec251e1c2719b28"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import sys\n",
    "if 'google.colab' in sys.modules:  # If in Google Colab environment\n",
    "    # Mount google drive to enable access to data files\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # Installing requisite packages\n",
    "    !pip install --upgrade transformers openai  &> /dev/null\n",
    "\n",
    "    # Change working directory \n",
    "    %cd /content/drive/MyDrive/LLM4BeSci_EADM2024/workshop_3"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm_notebook as tqdm\n",
    "from huggingface_hub import InferenceClient\n",
    "import textwrap"
   ],
   "id": "2948ad848d628827",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Zero-shot Classification: Media Bias\n",
    "The goal of this section will be to classify tweets as either `\"neutral\"` or `\"partisan\"`. We will make use of a [dataset](https://data.world/crowdflower/classification-of-pol-social) of tweets containing four columns:\n",
    "\n",
    "1. `\"author\"`: The author of the tweet.\n",
    "2. `\"text\"`: The text of the tweet.\n",
    "3. `\"bias\"`:  The political bias of the tweet. This can be either `\"neutral\"` or `\"partisan\"`. The number of neutral and partisan tweets is intentionally equal in the dataset.\n",
    "4. `\"type\"`:  The type of tweet.\n",
    "\n",
    "We begin by loading the dataset as a `pandas.DataFrame`:"
   ],
   "id": "714e83d6d0c68d78"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "media_bias = pd.read_csv('media_bias.csv')\n",
    "media_bias"
   ],
   "id": "b943e40b73abab85",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The code next initializes the `InferenceClient` with an access token, **which you will need to replace with your own [access token](https://huggingface.co/settings/tokens)** (access tokens start with 'hf_...'). It then loops through the tweets in the `media_bias` dataframe. The code then generates the output using the `chat_completion` method of the `InferenceClient` class. This allows us to play around with certain generation-related parameters such as `max_tokens` and `temperature`. \n",
    "\n",
    "The output is then parsed to extract the label, which is then appended to the `zero_shot_labels` list. The code then adds the `zero_shot_labels` list to the `media_bias` dataframe."
   ],
   "id": "de4f61741148117d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Initialize client\n",
    "api_key = '<your_access_token>' \n",
    "client = InferenceClient(token=api_key)\n",
    "\n",
    "# Zero-shot classification prompt\n",
    "zero_shot_prompt = \"Is this text neutral or partisan? Strictly answer with only 'neutral' or 'partisan':\\n\"\n",
    "\n",
    "zero_shot_labels = []\n",
    "for tweet in tqdm(media_bias['text']):    \n",
    "    \n",
    "    # Zero-shot classification \n",
    "    output = client.chat_completion(\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": zero_shot_prompt + tweet}\n",
    "        ],\n",
    "        model=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "        max_tokens=50,\n",
    "        temperature=0.0\n",
    "    )\n",
    "    \n",
    "    # Accessing the text output and lowercasing it\n",
    "    output = output.choices[0].message.content.lower()\n",
    "    \n",
    "    # Extract label and append to list\n",
    "    label = 'neutral' if 'neutral' in output else 'partisan' if 'partisan' in output else 'nan' # \n",
    "    zero_shot_labels.append(label)\n",
    "\n",
    "# Add zero-shot labels to dataframe\n",
    "media_bias['zero_shot_label'] = zero_shot_labels\n",
    "media_bias"
   ],
   "id": "ecf9f91c8f9b6e6c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Comparing zero-shot and actual labels\n",
    "print(f'Zero-shot accuracy: {(media_bias[\"zero_shot_label\"] == media_bias[\"bias\"]).mean()}')"
   ],
   "id": "1566347fad5c0d8d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Impressively, without any task-specific training, the model achieves an accuracy of 72%. This compares to a baseline accuracy of 50% that could be achieved by randomly guessing between the two classes.\n",
    "\n",
    "The results can also be visualized with a confusion matrix, which shows the number of true positives, true negatives, false positives, and false negatives. The confusion matrix can be used to identify where the model is making mistakes and to understand the types of errors."
   ],
   "id": "763143303054bc36"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Confusion matrix\n",
    "confusion = pd.crosstab(media_bias['bias'], media_bias['zero_shot_label'])\n",
    "sns.heatmap(confusion, annot=True)"
   ],
   "id": "28a14100899bbea",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "As you can see, the model has a bias towards classifying tweets as neutral, leading to a higher number of false negatives than false positives.\n",
    "\n",
    "**TASK 1**: Try playing around with the system prompt to give it different roles. Can you find one that increases the accuracy of Llama 3? (e.g. `\"You are a thoughtful political scientist who accurately distinguishes neutral and partisan messages\"`).<br>"
   ],
   "id": "a095b45c98303886"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Synthetic Participants: The Berlin Numeracy Test\n",
    "In this section, we will explore the usage of causal LLMs as synthetic participants in a psychological experiment. We will this time use the 70B parameter version of Llama 3 to solve the [Berlin Numeracy Test](https://doi.org/10.1017/S1930297500001819). This is a widely used test to measure an individual's ability to understand and apply statistical concepts. \n",
    "\n",
    "The test consists of four questions that require a basic understanding of probability and statistics. In this exercise, we will ask Llama 3 to solve these questions. Llama 3 will provide an answer to each question, and we will evaluate the quality of the response.\n",
    "\n",
    "The code begins by defining the four questions: "
   ],
   "id": "1b3fa1e8bebb0454"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "q1 = \"\"\"\n",
    "Imagine we are throwing a five-sided die 50 times. On average, out of these 50 throws how many times would this five-sided die show an odd number (1, 3 or 5)?\n",
    "\"\"\"\n",
    "\n",
    "q2 = \"\"\"\n",
    "Out of 1,000 people in a small town 500 are members of a choir. Out of these 500 members in the choir 100 are men. Out of the 500 inhabitants that are not in the choir 300 are men. What is the probability that a randomly drawn man is a member of the choir? (please indicate the probability in percent).\n",
    "\"\"\"\n",
    "\n",
    "q3 = \"\"\"\n",
    "Imagine we are throwing a loaded die (6 sides). The probability that the die shows a 6 is twice as high as the probability of each of the other numbers. On average, out of these 70 throws, how many times would the die show the number 6?\n",
    "\"\"\"\n",
    "\n",
    "q4 = \"\"\"\n",
    "In a forest 20% of mushrooms are red, 50% brown and 30% white. A red mushroom is poisonous with a probability of 20%. A mushroom that is not red is poisonous with a probability of 5%. What is the probability that a poisonous mushroom in the forest is red?\n",
    "\"\"\""
   ],
   "id": "29b04f4177d7b5e2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We will again use the HF InferenceClient to interact with the LLM. However, since the 70B parameter version of Llama 3 requires a HF Pro account, you will need to copy-paste the access token that we have provided for you as the `api_key` variable.",
   "id": "5e4267d9a3d09eed"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# api_key = '<pro_access_token>'\n",
    "\n",
    "# Loop through questions and generate responses\n",
    "for i, question in enumerate([q1, q2, q3, q4]):\n",
    "    print('-------------------------')   \n",
    "    \n",
    "    # Add additional instruction to the question\n",
    "    question += \"\"\"\n",
    "    Make your answer as brief as possible using less than 50 words.\n",
    "    Add ** around your your final answer to make it more visible.\n",
    "     \"\"\"\n",
    "    \n",
    "    output = client.chat_completion(\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are an average participant in a psychology experiment.\"},\n",
    "        {\"role\": \"user\", \"content\": question}\n",
    "    ],\n",
    "    model=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    max_tokens=1000,\n",
    "    temperature=0.0\n",
    "    )\n",
    "    \n",
    "    # Accessing the text output \n",
    "    response = output.choices[0].message.content\n",
    "    \n",
    "    # Format question and response for printing\n",
    "    question = '\\n'.join(textwrap.wrap(question, 100))\n",
    "    response = '\\n'.join(textwrap.wrap(response, 100))\n",
    "    print(f\"Question {i+1}: {question} \\n\\nAnswer: {response}\\n\")"
   ],
   "id": "71c9c283b1ebabf9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The correct answers are 30, 25, 20, and 50, meaning the model got 2/4 questions correct. \n",
    "\n",
    "**TASK 1**: Change the `temperature` value to `1.0`, `3.0`, and `float('inf')`. What impact does temperature have on the responses?<br>\n",
    "**TASK 2**: Change the `temperature` back to `0.0`. Now remove the instruction to `\"Make your answer as brief as possible using less than 50 words.\"` and replace it a *chain-of-thought* prompt instructing the model to `\"Go through your reasoning step by step before giving the final answer.\"`. Why do you think this might improve the quality of the responses? <br>\n",
    "**TASK 3**: Try changing the `model` to `\"meta-llama/Meta-Llama-3-70B-Instruct\"`. You will also need to uncomment the line with `# api_key = '<pro_access_token>'` and add the key we gave you. Does the larger model do any better?<br>"
   ],
   "id": "e97094deb99953c0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
