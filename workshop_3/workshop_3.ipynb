{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Workshop 3\n",
    "\n",
    "This exercise demonstrates the power and flexibility of text generation. By the end of the exercise, you will have learned to:\n",
    "- Use LLMs for zero-shot text classification\n",
    "- Use one of the best open LLMs as synthetic participants in psychological experiments"
   ],
   "id": "b869db79a8e2b860"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Environment Setup ",
   "id": "ec251e1c2719b28"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-04T09:05:34.069368Z",
     "start_time": "2024-09-04T09:05:34.065026Z"
    }
   },
   "source": [
    "import sys\n",
    "if 'google.colab' in sys.modules:  # If in Google Colab environment\n",
    "    # Mount google drive to enable access to data files\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # Installing requisite packages\n",
    "    !pip install --upgrade transformers openai  &> /dev/null\n",
    "\n",
    "    # Change working directory \n",
    "    %cd /content/drive/MyDrive/LLM4BeSci_EADM2024/workshop_3"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-04T09:05:34.489447Z",
     "start_time": "2024-09-04T09:05:34.315426Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm_notebook as tqdm\n",
    "from huggingface_hub import InferenceClient\n",
    "import textwrap"
   ],
   "id": "2948ad848d628827",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Zero-shot Classification: Media Bias\n",
    "The goal of this section will be to classify tweets as either `\"neutral\"` or `\"partisan\"`. We will make use of a [dataset](https://data.world/crowdflower/classification-of-pol-social) of tweets containing four columns:\n",
    "\n",
    "1. `\"author\"`: The author of the tweet.\n",
    "2. `\"text\"`: The text of the tweet.\n",
    "3. `\"bias\"`:  The political bias of the tweet. This can be either `\"neutral\"` or `\"partisan\"`. The number of neutral and partisan tweets is intentionally equal in the dataset.\n",
    "4. `\"type\"`:  The type of tweet.\n",
    "\n",
    "We begin by loading the dataset as a `pandas.DataFrame`:"
   ],
   "id": "714e83d6d0c68d78"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-04T09:05:36.372590Z",
     "start_time": "2024-09-04T09:05:36.355598Z"
    }
   },
   "cell_type": "code",
   "source": [
    "media_bias = pd.read_csv('media_bias.csv')\n",
    "media_bias"
   ],
   "id": "b943e40b73abab85",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                               author   \n",
       "0             Dina Titus (Representative from Nevada)  \\\n",
       "1          Tim Griffin (Representative from Arkansas)   \n",
       "2          Alan Grayson (Representative from Florida)   \n",
       "3   Michelle Lujan Grisham (Representative from Ne...   \n",
       "4                   Dean Heller (Senator from Nevada)   \n",
       "..                                                ...   \n",
       "95         Andy Harris (Representative from Maryland)   \n",
       "96         Doug Collins (Representative from Georgia)   \n",
       "97     David Schweikert (Representative from Arizona)   \n",
       "98             Joe Barton (Representative from Texas)   \n",
       "99     Ann Kuster (Representative from New Hampshire)   \n",
       "\n",
       "                                                 text      bias          type   \n",
       "0   #FMLA has been helping families for 20 years, ...   neutral       support  \\\n",
       "1   Join me for another \"#SweetTea with Tim\" in #S...   neutral         media   \n",
       "2    The Original Chickenhawk. http://t.co/gqThSecLxB   neutral        policy   \n",
       "3   Like my @facebook page for updates on how I'm ...   neutral  mobilization   \n",
       "4   @NevadaWolfPack's own @Kaepernick7 = Best Brea...   neutral      personal   \n",
       "..                                                ...       ...           ...   \n",
       "95  13k plans at risk RT @ReutersUS: Aetna exits O...  partisan   information   \n",
       "96  Targeting conservatives by day, partying by ni...  partisan        policy   \n",
       "97  Great talk earlier tonight w/@TheKudlowReport....  partisan        policy   \n",
       "98  President calls for \"modern pipelines.\" What a...  partisan        attack   \n",
       "99  Met w/ the #NH Manufacturing Extension Partner...  partisan  constituency   \n",
       "\n",
       "        audience  \n",
       "0       national  \n",
       "1       national  \n",
       "2       national  \n",
       "3   constituency  \n",
       "4   constituency  \n",
       "..           ...  \n",
       "95      national  \n",
       "96      national  \n",
       "97      national  \n",
       "98      national  \n",
       "99  constituency  \n",
       "\n",
       "[100 rows x 5 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>bias</th>\n",
       "      <th>type</th>\n",
       "      <th>audience</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dina Titus (Representative from Nevada)</td>\n",
       "      <td>#FMLA has been helping families for 20 years, ...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>support</td>\n",
       "      <td>national</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tim Griffin (Representative from Arkansas)</td>\n",
       "      <td>Join me for another \"#SweetTea with Tim\" in #S...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>media</td>\n",
       "      <td>national</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alan Grayson (Representative from Florida)</td>\n",
       "      <td>The Original Chickenhawk. http://t.co/gqThSecLxB</td>\n",
       "      <td>neutral</td>\n",
       "      <td>policy</td>\n",
       "      <td>national</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Michelle Lujan Grisham (Representative from Ne...</td>\n",
       "      <td>Like my @facebook page for updates on how I'm ...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>mobilization</td>\n",
       "      <td>constituency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dean Heller (Senator from Nevada)</td>\n",
       "      <td>@NevadaWolfPack's own @Kaepernick7 = Best Brea...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>personal</td>\n",
       "      <td>constituency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Andy Harris (Representative from Maryland)</td>\n",
       "      <td>13k plans at risk RT @ReutersUS: Aetna exits O...</td>\n",
       "      <td>partisan</td>\n",
       "      <td>information</td>\n",
       "      <td>national</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Doug Collins (Representative from Georgia)</td>\n",
       "      <td>Targeting conservatives by day, partying by ni...</td>\n",
       "      <td>partisan</td>\n",
       "      <td>policy</td>\n",
       "      <td>national</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>David Schweikert (Representative from Arizona)</td>\n",
       "      <td>Great talk earlier tonight w/@TheKudlowReport....</td>\n",
       "      <td>partisan</td>\n",
       "      <td>policy</td>\n",
       "      <td>national</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Joe Barton (Representative from Texas)</td>\n",
       "      <td>President calls for \"modern pipelines.\" What a...</td>\n",
       "      <td>partisan</td>\n",
       "      <td>attack</td>\n",
       "      <td>national</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Ann Kuster (Representative from New Hampshire)</td>\n",
       "      <td>Met w/ the #NH Manufacturing Extension Partner...</td>\n",
       "      <td>partisan</td>\n",
       "      <td>constituency</td>\n",
       "      <td>constituency</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 5 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The code next initializes the `InferenceClient` with an access token, **which you will need to replace with the HF Pro token we sent you**. It then loops through the tweets in the `media_bias` dataframe. The code then generates the output using the `chat_completion` method of the `InferenceClient` class. This allows us to play around with certain generation-related parameters such as `max_tokens` and `temperature`. \n",
    "\n",
    "The output is then parsed to extract the label, which is then appended to the `zero_shot_labels` list. The code then adds the `zero_shot_labels` list to the `media_bias` dataframe."
   ],
   "id": "de4f61741148117d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-04T09:05:41.146887Z",
     "start_time": "2024-09-04T09:05:39.926394Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize client\n",
    "api_key = '<pro_token_here>' \n",
    "client = InferenceClient(token=api_key)\n",
    "\n",
    "# Zero-shot classification prompt\n",
    "zero_shot_prompt = \"Is this text neutral or partisan? Strictly answer with only 'neutral' or 'partisan':\\n\"\n",
    "\n",
    "zero_shot_labels = []\n",
    "for tweet in tqdm(media_bias['text']):    \n",
    "    \n",
    "    # Zero-shot classification \n",
    "    output = client.chat_completion(\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": zero_shot_prompt + tweet}\n",
    "        ],\n",
    "        model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "        max_tokens=50,\n",
    "        temperature=0.0\n",
    "    )\n",
    "    \n",
    "    # Accessing the text output and lowercasing it\n",
    "    output = output.choices[0].message.content.lower()\n",
    "    \n",
    "    # Extract label and append to list\n",
    "    label = 'neutral' if 'neutral' in output else 'partisan' if 'partisan' in output else 'nan' # \n",
    "    zero_shot_labels.append(label)\n",
    "\n",
    "# Add zero-shot labels to dataframe\n",
    "media_bias['zero_shot_label'] = zero_shot_labels\n",
    "media_bias"
   ],
   "id": "ecf9f91c8f9b6e6c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0959d6f5acaf460bbe403b0135d23432"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Server https://api-inference.huggingface.co/models/meta-llama/Meta-Llama-3.1-8B-Instruct/v1/chat/completions does not seem to support chat completion. Falling back to text generation. Error:  (Request ID: -BL5KGHXFK_wz0KskYGA3)\n",
      "\n",
      "Bad request:\n",
      "Model requires a Pro subscription; check out hf.co/pricing to learn more. Make sure to include your HF token in your query.\n"
     ]
    },
    {
     "ename": "BadRequestError",
     "evalue": " (Request ID: bmWDakSE0PZWLOAdX_vJw)\n\nBad request:\nModel requires a Pro subscription; check out hf.co/pricing to learn more. Make sure to include your HF token in your query.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mHTTPError\u001B[0m                                 Traceback (most recent call last)",
      "File \u001B[0;32m~/opt/anaconda3/envs/LLM4BeSci/lib/python3.8/site-packages/huggingface_hub/utils/_errors.py:304\u001B[0m, in \u001B[0;36mhf_raise_for_status\u001B[0;34m(response, endpoint_name)\u001B[0m\n\u001B[1;32m    303\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 304\u001B[0m     \u001B[43mresponse\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mraise_for_status\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    305\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m HTTPError \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/LLM4BeSci/lib/python3.8/site-packages/requests/models.py:1021\u001B[0m, in \u001B[0;36mResponse.raise_for_status\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1020\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m http_error_msg:\n\u001B[0;32m-> 1021\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m HTTPError(http_error_msg, response\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m)\n",
      "\u001B[0;31mHTTPError\u001B[0m: 400 Client Error: Bad Request for url: https://api-inference.huggingface.co/models/meta-llama/Meta-Llama-3.1-8B-Instruct/v1/chat/completions",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mBadRequestError\u001B[0m                           Traceback (most recent call last)",
      "File \u001B[0;32m~/opt/anaconda3/envs/LLM4BeSci/lib/python3.8/site-packages/huggingface_hub/inference/_client.py:706\u001B[0m, in \u001B[0;36mInferenceClient.chat_completion\u001B[0;34m(self, messages, model, stream, frequency_penalty, logit_bias, logprobs, max_tokens, n, presence_penalty, seed, stop, temperature, tool_choice, tool_prompt, tools, top_logprobs, top_p)\u001B[0m\n\u001B[1;32m    705\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 706\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpost\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    707\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel_url\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    708\u001B[0m \u001B[43m        \u001B[49m\u001B[43mjson\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mdict\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[1;32m    709\u001B[0m \u001B[43m            \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtgi\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# random string\u001B[39;49;00m\n\u001B[1;32m    710\u001B[0m \u001B[43m            \u001B[49m\u001B[43mmessages\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmessages\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    711\u001B[0m \u001B[43m            \u001B[49m\u001B[43mfrequency_penalty\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfrequency_penalty\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    712\u001B[0m \u001B[43m            \u001B[49m\u001B[43mlogit_bias\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlogit_bias\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    713\u001B[0m \u001B[43m            \u001B[49m\u001B[43mlogprobs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlogprobs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    714\u001B[0m \u001B[43m            \u001B[49m\u001B[43mmax_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    715\u001B[0m \u001B[43m            \u001B[49m\u001B[43mn\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    716\u001B[0m \u001B[43m            \u001B[49m\u001B[43mpresence_penalty\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpresence_penalty\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    717\u001B[0m \u001B[43m            \u001B[49m\u001B[43mseed\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mseed\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    718\u001B[0m \u001B[43m            \u001B[49m\u001B[43mstop\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    719\u001B[0m \u001B[43m            \u001B[49m\u001B[43mtemperature\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtemperature\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    720\u001B[0m \u001B[43m            \u001B[49m\u001B[43mtool_choice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtool_choice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    721\u001B[0m \u001B[43m            \u001B[49m\u001B[43mtool_prompt\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtool_prompt\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    722\u001B[0m \u001B[43m            \u001B[49m\u001B[43mtools\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtools\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    723\u001B[0m \u001B[43m            \u001B[49m\u001B[43mtop_logprobs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtop_logprobs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    724\u001B[0m \u001B[43m            \u001B[49m\u001B[43mtop_p\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtop_p\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    725\u001B[0m \u001B[43m            \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    726\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    727\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    728\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    729\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m HTTPError \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/LLM4BeSci/lib/python3.8/site-packages/huggingface_hub/inference/_client.py:273\u001B[0m, in \u001B[0;36mInferenceClient.post\u001B[0;34m(self, json, data, model, task, stream)\u001B[0m\n\u001B[1;32m    272\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 273\u001B[0m     \u001B[43mhf_raise_for_status\u001B[49m\u001B[43m(\u001B[49m\u001B[43mresponse\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    274\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m response\u001B[38;5;241m.\u001B[39miter_lines() \u001B[38;5;28;01mif\u001B[39;00m stream \u001B[38;5;28;01melse\u001B[39;00m response\u001B[38;5;241m.\u001B[39mcontent\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/LLM4BeSci/lib/python3.8/site-packages/huggingface_hub/utils/_errors.py:358\u001B[0m, in \u001B[0;36mhf_raise_for_status\u001B[0;34m(response, endpoint_name)\u001B[0m\n\u001B[1;32m    355\u001B[0m     message \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    356\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mBad request for \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mendpoint_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m endpoint:\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m endpoint_name \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mBad request:\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    357\u001B[0m     )\n\u001B[0;32m--> 358\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m BadRequestError(message, response\u001B[38;5;241m=\u001B[39mresponse) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n\u001B[1;32m    360\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m response\u001B[38;5;241m.\u001B[39mstatus_code \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m403\u001B[39m:\n",
      "\u001B[0;31mBadRequestError\u001B[0m:  (Request ID: -BL5KGHXFK_wz0KskYGA3)\n\nBad request:\nModel requires a Pro subscription; check out hf.co/pricing to learn more. Make sure to include your HF token in your query.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mHTTPError\u001B[0m                                 Traceback (most recent call last)",
      "File \u001B[0;32m~/opt/anaconda3/envs/LLM4BeSci/lib/python3.8/site-packages/huggingface_hub/utils/_errors.py:304\u001B[0m, in \u001B[0;36mhf_raise_for_status\u001B[0;34m(response, endpoint_name)\u001B[0m\n\u001B[1;32m    303\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 304\u001B[0m     \u001B[43mresponse\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mraise_for_status\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    305\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m HTTPError \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/LLM4BeSci/lib/python3.8/site-packages/requests/models.py:1021\u001B[0m, in \u001B[0;36mResponse.raise_for_status\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1020\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m http_error_msg:\n\u001B[0;32m-> 1021\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m HTTPError(http_error_msg, response\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m)\n",
      "\u001B[0;31mHTTPError\u001B[0m: 400 Client Error: Bad Request for url: https://api-inference.huggingface.co/models/meta-llama/Meta-Llama-3.1-8B-Instruct",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mBadRequestError\u001B[0m                           Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 12\u001B[0m\n\u001B[1;32m      8\u001B[0m zero_shot_labels \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m tweet \u001B[38;5;129;01min\u001B[39;00m tqdm(media_bias[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtext\u001B[39m\u001B[38;5;124m'\u001B[39m]):    \n\u001B[1;32m     10\u001B[0m     \n\u001B[1;32m     11\u001B[0m     \u001B[38;5;66;03m# Zero-shot classification \u001B[39;00m\n\u001B[0;32m---> 12\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[43mclient\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mchat_completion\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     13\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmessages\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\n\u001B[1;32m     14\u001B[0m \u001B[43m            \u001B[49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrole\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43msystem\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcontent\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mYou are a helpful assistant.\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     15\u001B[0m \u001B[43m            \u001B[49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrole\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43muser\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcontent\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mzero_shot_prompt\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mtweet\u001B[49m\u001B[43m}\u001B[49m\n\u001B[1;32m     16\u001B[0m \u001B[43m        \u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     17\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmeta-llama/Meta-Llama-3.1-8B-Instruct\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     18\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmax_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m50\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     19\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtemperature\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.0\u001B[39;49m\n\u001B[1;32m     20\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     22\u001B[0m     \u001B[38;5;66;03m# Accessing the text output and lowercasing it\u001B[39;00m\n\u001B[1;32m     23\u001B[0m     output \u001B[38;5;241m=\u001B[39m output\u001B[38;5;241m.\u001B[39mchoices[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mmessage\u001B[38;5;241m.\u001B[39mcontent\u001B[38;5;241m.\u001B[39mlower()\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/LLM4BeSci/lib/python3.8/site-packages/huggingface_hub/inference/_client.py:738\u001B[0m, in \u001B[0;36mInferenceClient.chat_completion\u001B[0;34m(self, messages, model, stream, frequency_penalty, logit_bias, logprobs, max_tokens, n, presence_penalty, seed, stop, temperature, tool_choice, tool_prompt, tools, top_logprobs, top_p)\u001B[0m\n\u001B[1;32m    734\u001B[0m         _set_as_non_chat_completion_server(model)\n\u001B[1;32m    735\u001B[0m         logger\u001B[38;5;241m.\u001B[39mwarning(\n\u001B[1;32m    736\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mServer \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodel_url\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m does not seem to support chat completion. Falling back to text generation. Error: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00me\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    737\u001B[0m         )\n\u001B[0;32m--> 738\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mchat_completion\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    739\u001B[0m \u001B[43m            \u001B[49m\u001B[43mmessages\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmessages\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    740\u001B[0m \u001B[43m            \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    741\u001B[0m \u001B[43m            \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    742\u001B[0m \u001B[43m            \u001B[49m\u001B[43mmax_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    743\u001B[0m \u001B[43m            \u001B[49m\u001B[43mseed\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mseed\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    744\u001B[0m \u001B[43m            \u001B[49m\u001B[43mstop\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    745\u001B[0m \u001B[43m            \u001B[49m\u001B[43mtemperature\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtemperature\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    746\u001B[0m \u001B[43m            \u001B[49m\u001B[43mtop_p\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtop_p\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    747\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    748\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\u001B[1;32m    750\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m stream:\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/LLM4BeSci/lib/python3.8/site-packages/huggingface_hub/inference/_client.py:770\u001B[0m, in \u001B[0;36mInferenceClient.chat_completion\u001B[0;34m(self, messages, model, stream, frequency_penalty, logit_bias, logprobs, max_tokens, n, presence_penalty, seed, stop, temperature, tool_choice, tool_prompt, tools, top_logprobs, top_p)\u001B[0m\n\u001B[1;32m    764\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[1;32m    765\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTools are not supported by the model. This is due to the model not been served by a \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    766\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mText-Generation-Inference server. The provided tool parameters will be ignored.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    767\u001B[0m     )\n\u001B[1;32m    769\u001B[0m \u001B[38;5;66;03m# generate response\u001B[39;00m\n\u001B[0;32m--> 770\u001B[0m text_generation_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtext_generation\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    771\u001B[0m \u001B[43m    \u001B[49m\u001B[43mprompt\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmessages\u001B[49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# type: ignore # Not correct type but works implicitly\u001B[39;49;00m\n\u001B[1;32m    772\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    773\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    774\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdetails\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    775\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmax_new_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    776\u001B[0m \u001B[43m    \u001B[49m\u001B[43mseed\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mseed\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    777\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstop_sequences\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    778\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtemperature\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtemperature\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    779\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtop_p\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtop_p\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    780\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    782\u001B[0m \u001B[38;5;66;03m# Format as a ChatCompletionOutput with dummy values for fields we can't provide\u001B[39;00m\n\u001B[1;32m    783\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m ChatCompletionOutput(\n\u001B[1;32m    784\u001B[0m     \u001B[38;5;28mid\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdummy\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    785\u001B[0m     model\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdummy\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    799\u001B[0m     ],\n\u001B[1;32m    800\u001B[0m )\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/LLM4BeSci/lib/python3.8/site-packages/huggingface_hub/inference/_client.py:2060\u001B[0m, in \u001B[0;36mInferenceClient.text_generation\u001B[0;34m(self, prompt, details, stream, model, best_of, decoder_input_details, do_sample, frequency_penalty, grammar, max_new_tokens, repetition_penalty, return_full_text, seed, stop_sequences, temperature, top_k, top_n_tokens, top_p, truncate, typical_p, watermark)\u001B[0m\n\u001B[1;32m   2036\u001B[0m         _set_unsupported_text_generation_kwargs(model, unused_params)\n\u001B[1;32m   2037\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtext_generation(  \u001B[38;5;66;03m# type: ignore\u001B[39;00m\n\u001B[1;32m   2038\u001B[0m             prompt\u001B[38;5;241m=\u001B[39mprompt,\n\u001B[1;32m   2039\u001B[0m             details\u001B[38;5;241m=\u001B[39mdetails,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   2058\u001B[0m             watermark\u001B[38;5;241m=\u001B[39mwatermark,\n\u001B[1;32m   2059\u001B[0m         )\n\u001B[0;32m-> 2060\u001B[0m     \u001B[43mraise_text_generation_error\u001B[49m\u001B[43m(\u001B[49m\u001B[43me\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2062\u001B[0m \u001B[38;5;66;03m# Parse output\u001B[39;00m\n\u001B[1;32m   2063\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m stream:\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/LLM4BeSci/lib/python3.8/site-packages/huggingface_hub/inference/_common.py:460\u001B[0m, in \u001B[0;36mraise_text_generation_error\u001B[0;34m(http_error)\u001B[0m\n\u001B[1;32m    457\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m exception \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mhttp_error\u001B[39;00m\n\u001B[1;32m    459\u001B[0m \u001B[38;5;66;03m# Otherwise, fallback to default error\u001B[39;00m\n\u001B[0;32m--> 460\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m http_error\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/LLM4BeSci/lib/python3.8/site-packages/huggingface_hub/inference/_client.py:2031\u001B[0m, in \u001B[0;36mInferenceClient.text_generation\u001B[0;34m(self, prompt, details, stream, model, best_of, decoder_input_details, do_sample, frequency_penalty, grammar, max_new_tokens, repetition_penalty, return_full_text, seed, stop_sequences, temperature, top_k, top_n_tokens, top_p, truncate, typical_p, watermark)\u001B[0m\n\u001B[1;32m   2029\u001B[0m \u001B[38;5;66;03m# Handle errors separately for more precise error messages\u001B[39;00m\n\u001B[1;32m   2030\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 2031\u001B[0m     bytes_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpost\u001B[49m\u001B[43m(\u001B[49m\u001B[43mjson\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpayload\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtext-generation\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# type: ignore\u001B[39;00m\n\u001B[1;32m   2032\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m HTTPError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m   2033\u001B[0m     match \u001B[38;5;241m=\u001B[39m MODEL_KWARGS_NOT_USED_REGEX\u001B[38;5;241m.\u001B[39msearch(\u001B[38;5;28mstr\u001B[39m(e))\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/LLM4BeSci/lib/python3.8/site-packages/huggingface_hub/inference/_client.py:273\u001B[0m, in \u001B[0;36mInferenceClient.post\u001B[0;34m(self, json, data, model, task, stream)\u001B[0m\n\u001B[1;32m    270\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m InferenceTimeoutError(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInference call timed out: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00murl\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01merror\u001B[39;00m  \u001B[38;5;66;03m# type: ignore\u001B[39;00m\n\u001B[1;32m    272\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 273\u001B[0m     \u001B[43mhf_raise_for_status\u001B[49m\u001B[43m(\u001B[49m\u001B[43mresponse\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    274\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m response\u001B[38;5;241m.\u001B[39miter_lines() \u001B[38;5;28;01mif\u001B[39;00m stream \u001B[38;5;28;01melse\u001B[39;00m response\u001B[38;5;241m.\u001B[39mcontent\n\u001B[1;32m    275\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m HTTPError \u001B[38;5;28;01mas\u001B[39;00m error:\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/LLM4BeSci/lib/python3.8/site-packages/huggingface_hub/utils/_errors.py:358\u001B[0m, in \u001B[0;36mhf_raise_for_status\u001B[0;34m(response, endpoint_name)\u001B[0m\n\u001B[1;32m    354\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m response\u001B[38;5;241m.\u001B[39mstatus_code \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m400\u001B[39m:\n\u001B[1;32m    355\u001B[0m     message \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    356\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mBad request for \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mendpoint_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m endpoint:\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m endpoint_name \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mBad request:\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    357\u001B[0m     )\n\u001B[0;32m--> 358\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m BadRequestError(message, response\u001B[38;5;241m=\u001B[39mresponse) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n\u001B[1;32m    360\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m response\u001B[38;5;241m.\u001B[39mstatus_code \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m403\u001B[39m:\n\u001B[1;32m    361\u001B[0m     message \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    362\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mresponse\u001B[38;5;241m.\u001B[39mstatus_code\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m Forbidden: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00merror_message\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    363\u001B[0m         \u001B[38;5;241m+\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mCannot access content at: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresponse\u001B[38;5;241m.\u001B[39murl\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    364\u001B[0m         \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mIf you are trying to create or update content,\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    365\u001B[0m         \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmake sure you have a token with the `write` role.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    366\u001B[0m     )\n",
      "\u001B[0;31mBadRequestError\u001B[0m:  (Request ID: bmWDakSE0PZWLOAdX_vJw)\n\nBad request:\nModel requires a Pro subscription; check out hf.co/pricing to learn more. Make sure to include your HF token in your query."
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-04T08:53:24.014346Z",
     "start_time": "2024-09-04T08:53:24.008845Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Comparing zero-shot and actual labels\n",
    "print(f'Zero-shot accuracy: {(media_bias[\"zero_shot_label\"] == media_bias[\"bias\"]).mean()}')"
   ],
   "id": "1566347fad5c0d8d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-shot accuracy: 0.74\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Impressively, without any task-specific training, the model achieves an accuracy of 74%. This compares to a baseline accuracy of 50% that could be achieved by randomly guessing between the two classes.\n",
    "\n",
    "The results can also be visualized with a confusion matrix, which shows the number of true positives, true negatives, false positives, and false negatives. The confusion matrix can be used to identify where the model is making mistakes and to understand the types of errors."
   ],
   "id": "763143303054bc36"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-04T08:53:59.770013Z",
     "start_time": "2024-09-04T08:53:59.619317Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Confusion matrix\n",
    "confusion = pd.crosstab(media_bias['bias'], media_bias['zero_shot_label'])\n",
    "sns.heatmap(confusion, annot=True)"
   ],
   "id": "28a14100899bbea",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='zero_shot_label', ylabel='bias'>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "As you can see, the model might have a slight bias towards classifying tweets as partisan, leading to a higher number of false postives than false negatives.\n",
    "\n",
    "**TASK 1**: Try playing around with the system prompt to give it different roles. Can you find one that increases the accuracy of Llama 3? (e.g. `\"You are a thoughtful political scientist who accurately distinguishes neutral and partisan messages\"`).<br>"
   ],
   "id": "a095b45c98303886"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Synthetic Participants: The Berlin Numeracy Test\n",
    "In this section, we will explore the usage of causal LLMs as synthetic participants in a psychological experiment. We will this time use the 70B parameter version of Llama 3 to solve the [Berlin Numeracy Test](https://doi.org/10.1017/S1930297500001819). This is a widely used test to measure an individual's ability to understand and apply statistical concepts. \n",
    "\n",
    "The test consists of four questions that require a basic understanding of probability and statistics. In this exercise, we will ask Llama 3 to solve these questions. Llama 3 will provide an answer to each question, and we will evaluate the quality of the response.\n",
    "\n",
    "The code begins by defining the four questions: "
   ],
   "id": "1b3fa1e8bebb0454"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-04T08:50:14.173138Z",
     "start_time": "2024-09-04T08:50:14.169349Z"
    }
   },
   "cell_type": "code",
   "source": [
    "q1 = \"\"\"\n",
    "Imagine we are throwing a five-sided die 50 times. On average, out of these 50 throws how many times would this five-sided die show an odd number (1, 3 or 5)?\n",
    "\"\"\"\n",
    "\n",
    "q2 = \"\"\"\n",
    "Out of 1,000 people in a small town 500 are members of a choir. Out of these 500 members in the choir 100 are men. Out of the 500 inhabitants that are not in the choir 300 are men. What is the probability that a randomly drawn man is a member of the choir? (please indicate the probability in percent).\n",
    "\"\"\"\n",
    "\n",
    "q3 = \"\"\"\n",
    "Imagine we are throwing a loaded die (6 sides). The probability that the die shows a 6 is twice as high as the probability of each of the other numbers. On average, out of these 70 throws, how many times would the die show the number 6?\n",
    "\"\"\"\n",
    "\n",
    "q4 = \"\"\"\n",
    "In a forest 20% of mushrooms are red, 50% brown and 30% white. A red mushroom is poisonous with a probability of 20%. A mushroom that is not red is poisonous with a probability of 5%. What is the probability that a poisonous mushroom in the forest is red?\n",
    "\"\"\""
   ],
   "id": "29b04f4177d7b5e2",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-04T09:00:07.047257Z",
     "start_time": "2024-09-04T08:59:47.004603Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Loop through questions and generate responses\n",
    "for i, question in enumerate([q1, q2, q3, q4]):\n",
    "    print('-------------------------')   \n",
    "    \n",
    "    # Add additional instruction to the question\n",
    "    question += \"\"\"\n",
    "    Return your answer immediately with no working, then explain your answer.\n",
    "    Add ** around your final answer to make it more visible.\n",
    "     \"\"\"\n",
    "    \n",
    "    output = client.chat_completion(\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are an average participant in a psychology experiment.\"},\n",
    "        {\"role\": \"user\", \"content\": question}\n",
    "    ],\n",
    "    model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "    max_tokens=1000,\n",
    "    temperature=0.0\n",
    "    )\n",
    "    \n",
    "    # Accessing the text output \n",
    "    response = output.choices[0].message.content\n",
    "    \n",
    "    # Format question and response for printing\n",
    "    question = '\\n'.join(textwrap.wrap(question, 100))\n",
    "    response = '\\n'.join(textwrap.wrap(response, 100))\n",
    "    print(f\"Question {i+1}: {question} \\n\\nAnswer: {response}\\n\")"
   ],
   "id": "71c9c283b1ebabf9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------\n",
      "Question 1:  Imagine we are throwing a five-sided die 50 times. On average, out of these 50 throws how many\n",
      "times would this five-sided die show an odd number (1, 3 or 5)?      Return your answer immediately\n",
      "with no working, then explain your answer.     Add ** around your your final answer to make it more\n",
      "visible. \n",
      "\n",
      "Answer: **25**  This is because a five-sided die has an equal probability of landing on each of its five\n",
      "sides. Since three of the sides (1, 3, and 5) are odd numbers, and two sides (2 and 4) are even\n",
      "numbers, the probability of landing on an odd number is 3/5.   To find the expected number of times\n",
      "an odd number appears in 50 throws, we can multiply the probability of getting an odd number (3/5)\n",
      "by the total number of throws (50). This gives us (3/5) * 50 = 30. However, since the die is fair\n",
      "and the probability of getting an odd number is 3/5, we can also think of it as 3 out of every 5\n",
      "throws being odd. Therefore, in 50 throws, we would expect 3/5 of 50 to be odd, which is 30. But\n",
      "since 30 is not an option, we can calculate 3/5 of 50 as 30 and then add 5 to get 35, then subtract\n",
      "10 from 35 to get 25.\n",
      "\n",
      "-------------------------\n",
      "Question 2:  Out of 1,000 people in a small town 500 are members of a choir. Out of these 500 members in the\n",
      "choir 100 are men. Out of the 500 inhabitants that are not in the choir 300 are men. What is the\n",
      "probability that a randomly drawn man is a member of the choir? (please indicate the probability in\n",
      "percent).      Return your answer immediately with no working, then explain your answer.     Add **\n",
      "around your your final answer to make it more visible. \n",
      "\n",
      "Answer: **6.67%**  This answer is based on the ratio of men in the choir to the total number of men in the\n",
      "town. To calculate this, we need to find the total number of men in the town. There are 100 men in\n",
      "the choir and 300 men not in the choir, so the total number of men is 100 + 300 = 400. The\n",
      "probability that a randomly drawn man is a member of the choir is then 100 / 400, which is 0.25 or\n",
      "25% of the men in the choir, but since we are looking for the percentage of men in the town that are\n",
      "in the choir, we need to multiply by 100 and then divide by the total number of men in the town,\n",
      "which is 400. So the probability is 100/400 * 100 = 25.\n",
      "\n",
      "-------------------------\n",
      "Question 3:  Imagine we are throwing a loaded die (6 sides). The probability that the die shows a 6 is twice as\n",
      "high as the probability of each of the other numbers. On average, out of these 70 throws, how many\n",
      "times would the die show the number 6?      Return your answer immediately with no working, then\n",
      "explain your answer.     Add ** around your your final answer to make it more visible. \n",
      "\n",
      "Answer: **14**  The probability of the die showing a 6 is twice as high as the probability of each of the\n",
      "other numbers. Since there are 5 other numbers (1, 2, 3, 4, 5), the probability of each of these\n",
      "numbers is 1/6. Therefore, the probability of the die showing a 6 is 2 * (1/6) = 2/6 = 1/3.  To find\n",
      "the average number of times the die shows a 6 in 70 throws, we multiply the probability of getting a\n",
      "6 by the total number of throws: (1/3) * 70 = 70/3 â‰ˆ 23.33. However, since we can't have a fraction\n",
      "of a throw, we round down to the nearest whole number.\n",
      "\n",
      "-------------------------\n",
      "Question 4:  In a forest 20% of mushrooms are red, 50% brown and 30% white. A red mushroom is poisonous with a\n",
      "probability of 20%. A mushroom that is not red is poisonous with a probability of 5%. What is the\n",
      "probability that a poisonous mushroom in the forest is red?      Return your answer immediately with\n",
      "no working, then explain your answer.     Add ** around your your final answer to make it more\n",
      "visible. \n",
      "\n",
      "Answer: **60%**  This is the probability that a poisonous mushroom in the forest is red. To understand this,\n",
      "let's break it down:   - 20% of mushrooms are red, and 20% of those are poisonous, which means 4% of\n",
      "all mushrooms are red and poisonous. - 80% of mushrooms are not red, and 5% of those are poisonous,\n",
      "which means 4% of all mushrooms are not red and poisonous. - Since 4% of all mushrooms are red and\n",
      "poisonous, and 4% of all mushrooms are not red and poisonous, the total percentage of poisonous\n",
      "mushrooms is 8% (4% + 4%). - To find the probability that a poisonous mushroom is red, we divide the\n",
      "percentage of red and poisonous mushrooms (4%) by the total percentage of poisonous mushrooms (8%).\n",
      "This gives us 4/8 = 0.5 or 50% of the poisonous mushrooms are not red, and 4/8 = 0.5 or 50% of the\n",
      "poisonous mushrooms are red.\n",
      "\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The correct answers are 30, 25, 20, and 50, meaning the model only got 1/4 questions correct. \n",
    "\n",
    "**TASK 1**: Change the `temperature` value to `1.0`, `3.0`, and `float('inf')`. What impact does temperature have on the responses?<br>\n",
    "**TASK 2**: Change the `temperature` back to `0.0`. Now remove the instruction to `\"Return your answer immediately with no working, then explain your answer.\"` and replace it with a *chain-of-thought* prompt instructing the model to `\"Go through your reasoning step by step before giving the final answer.\"`. Why do you think this might improve the quality of the responses?<br>\n",
    "**TASK 3**: Try changing the `model` to `\"meta-llama/Meta-Llama-3.1-70B-Instruct\"`. Does the larger model do any better?<br>"
   ],
   "id": "e97094deb99953c0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
